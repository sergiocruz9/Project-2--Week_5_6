# Project-2--Week_5_6

Vanguard A/B Test Analysis
Analysis of Vanguard's digital interface redesign experiment to determine if a new UI design improves user completion rates and overall experience.

Project Overview
Period: March 15, 2017 - June 20, 2017
Objective: Evaluate whether a re designed online process increases client completion rates by at least 5%
Method: A/B testing - Control Group vs Test Group

Team Members:
Jeremy Patole
Sergio Cruz

Key Findings
Completion Rate: 
  Statistical Significance:
Error rates: 
  Statistical Significance: 
Cost-Effectiveness: 

Datasets

Client Demographics (df_final_demo): Age, gender, tenure, account details
Web Interactions (df_final_web_data_pt_1, df_final_web_data_pt_2): User activity logs
Experiment Roster (df_final_experiment_clients): Test/Control group assignments

Key Performance Indicators (KPIs)
Completion Rate: % of users reaching the final 'confirm' step
Time Spent: Average duration for completing the process
Error Rate: % of backward steps by client

Project Workflow
Data Cleaning & EDA → Understand client demographics and behavior
Performance Metrics → Calculate and compare KPIs
Hypothesis Testing → Statistical validation of results
Experiment Evaluation → Analyse design performance
Visualization → Tableau dashboard for interactive exploration

Hypothesis Tests Conducted
Completion Rate Difference (Two-proportion z-test)
Error rate difference (Chi square test)
Cost-Effectiveness Threshold (One-sided z-test, 5% minimum improvement)

 Data Quality Issues
Missing User Variation Group Assignment: 
Missing Gender Data: 

Visualizations
Interactive Tableau dashboard available at: TABLEAU LINK

Dashboard Features:

Completion rate comparison
Time spent analysis by step
Error rate visualization
Demographic filters (age, gender, tenure)

Links
Presentation Slides: 
Tableau Public: 


